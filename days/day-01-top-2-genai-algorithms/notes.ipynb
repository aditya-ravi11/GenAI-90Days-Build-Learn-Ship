{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed15750d",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe9eb2",
   "metadata": {},
   "source": [
    "**Topic:** Top 2 GenAI Algorithms\n",
    "\n",
    "**Why:** To gain basic understanding of GenAI, and 2 main algorithms of GenAI\n",
    "\n",
    "**What I verified today:**\n",
    "The concept of GenAI. \n",
    "- 2 Types of GenAI algorithms:\n",
    "1. Generative adversial networks (GANs):\n",
    "\n",
    "    - It consists of a generator network and a discriminator network that are trained simultaneously through adversarial learning.\n",
    "    - What is Adversarial Learning?- It means to oppose each other, so adversial training is a method used to improve the robustness and generalization of neural networks by incorporating adversarial or opposing examples that can contradict each other so the model can learn better.\n",
    "    - The generator aims to create realistic data whereas the discriminators role is to distinguish between real and generated data. This dynamic creates a competitive process that ultimately results in the generation of highly convincing synthetic content/data.\n",
    "2. Variational autoencoders (VAEs):\n",
    "\n",
    "    - Variational autoencoders take a probabilistic approach to generative modelling.\n",
    "    - They learn the underlying distribution of the data and use it to generate new samples.\n",
    "    - VAEs are are characterized by an encoder-decoder architecture, where the encoder maps the input data to a probabilistic distribution and the decoder samples from this distribution to generate new samples.\n",
    "\n",
    "\n",
    "- The key differences between GANs and VAEs:\n",
    "\n",
    "|GANs|VAEs|\n",
    "| --- | --- |\n",
    "|GANs consist of 2 neural networks- a generator and a discriminator that are trained simultaneously using adversarial training. The generator creates synthetic data and the discriminator tries to distinguish between real and generated data.|VAEs have an encoder-decoder architecture. The encoder maps input data to a probabilistic distribution in a latent space and the decoder samples from this distribution to generate new data.|\n",
    "|The generator aims to generate data that is indistinguishable from the real data while the discriminator aims to improve at distinguishing between real and generated data.|VAEs maximize the likelihood of generating the input data while ensuring the learned latent space follows a specific probabilistic distribution (usually Gaussian Distribution).|\n",
    "|GANs donot specifically define a structured latent space. The latent space is considered a byproduct of adversarial training, and may not have clear interpretability.|VAEs explicitly model a latent space with a well defined structure. The encoder maps input data to the distribution in this latent space, allowing for interpolation and manipulation of the latent representations.|\n",
    "|GANs are known for generating high quality and visually realistic samples. They excel in capturing complex patterns in the data and producing sharp, coherent images.|VAEs tend to generate samples that may be less visually realistic compared to GANs, but they are often more diverse. VAEs are good at exploring the latent space and generating a range of samples from the a given distribution.|\n",
    "|GANs are widely used in tasks such as image-to-image translation, style transfer and the generation of realistic images, videos or even text.|VAEs find applications in tasks where a structured latent space is beneficial such as image generation, variational image synthesis and data augmentation in healthcare applications.|\n",
    "\n",
    "- Applications of GenAI:\n",
    "1. arts and creativity\n",
    "2. medical imaging\n",
    "3. gaming\n",
    "4. content generation\n",
    "\n",
    "- Advancements of GenAI:\n",
    "    - Reinforcement Learning with Human Feedback (RLHF):\n",
    "        - This approach leverages human input to guide the learning process of LLMs, allowing them to continuously improve and adapt to evolving needs. Ensuring it stays on track and delivers results you desire.\n",
    "        - LoRA and QLoRA: These techniques provide a lightweight and efficient way to fine-tune LLMs for specific tasks. \n",
    "    - Improved Multimodality: \n",
    "        - LLMs are becoming increasingly adept at handling multiple data modalities like text, audio and images. \n",
    "        - This opens up future possibilties such as generating music (audio) from paintings/drawings (image) and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# tiny check: numerical stability of softmax\n",
    "x = np.array([1000.0, 999.0, 998.0])\n",
    "x = x - x.max()\n",
    "p = np.exp(x) / np.exp(x).sum()\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af867ff9",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- ...\n",
    "\n",
    "**Next steps:**\n",
    "- ..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
